{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import numpy as np\n",
    "import random\n",
    "import audiomentations as A\n",
    "import soundfile as sf\n",
    "import os\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for data preprocessing\n",
    "num_mfcc_coefficients = 13  # Number of MFCC coefficients (adjust as needed)\n",
    "desired_shape = (408, num_mfcc_coefficients, 1)  # Replace with your model's input shape\n",
    "main_folder = ['belly_pain', 'burping', 'discomfort', 'hungry', 'tired']  # Replace with your class names\n",
    "folder_name = 'donateacry'  # Replace with your dataset folder path\n",
    "num_classes = len(main_folder)  # Number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation, Data pre-processing & Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess an audio file\n",
    "def preprocess_audio(audio_folder):\n",
    "    preprocessed_data = []\n",
    "    preprocessed_labels = []\n",
    "    for index, cls in enumerate(audio_folder):\n",
    "        class_folder = os.path.join(folder_name, cls)\n",
    "        for file in os.listdir(class_folder)[:15]:\n",
    "            audio_file = os.path.join(class_folder, file)\n",
    "            original_audio, sr = sf.read(audio_file)\n",
    "            # Define the augmentation pipeline\n",
    "            augment1 = A.Compose([A.AddGaussianNoise(p=0.2)])\n",
    "            augment2 = A.Compose([A.TimeStretch(p=0.2)])\n",
    "            augment3 = A.Compose([A.PitchShift(p=0.2)])\n",
    "            augment4 = A.Compose([A.Shift(p=0.2)])\n",
    "            augment5 = A.Compose([A.TimeMask(p=0.2)])\n",
    "\n",
    "            # Apply augmentation to create augmented audio\n",
    "            augmented_audio1 = augment1(samples=original_audio, sample_rate=sr)\n",
    "            augmented_audio2 = augment2(samples=original_audio, sample_rate=sr)\n",
    "            augmented_audio3 = augment3(samples=original_audio, sample_rate=sr)\n",
    "            augmented_audio4 = augment4(samples=original_audio, sample_rate=sr)\n",
    "            augmented_audio5 = augment5(samples=original_audio, sample_rate=sr)\n",
    "            \n",
    "\n",
    "            # Perform feature extraction (e.g., MFCCs)\n",
    "            for audio in [original_audio, augmented_audio1, augmented_audio2, augmented_audio3, augmented_audio4, augmented_audio5]:\n",
    "                mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=num_mfcc_coefficients)\n",
    "\n",
    "                # Normalize the MFCCs (optional but recommended)\n",
    "                mfccs = (mfccs - np.mean(mfccs)) / np.std(mfccs)\n",
    "\n",
    "                # Reshape or pad the MFCCs to match the desired input shape\n",
    "                num_frames = mfccs.shape[1]\n",
    "                if num_frames < desired_shape[0]:\n",
    "                    mfccs = np.pad(mfccs, ((0, 0), (0, desired_shape[0] - num_frames)), mode='constant')\n",
    "                elif num_frames > desired_shape[0]:\n",
    "                    mfccs = mfccs[:, :desired_shape[0]]\n",
    "\n",
    "                # Append the preprocessed data and label\n",
    "                preprocessed_data.append(mfccs.T[:, :, np.newaxis])  # Transpose the data\n",
    "                preprocessed_labels.append(index)\n",
    "\n",
    "    return np.array(preprocessed_data),np.array(preprocessed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files: 408\n",
      "Total labels: 408\n"
     ]
    }
   ],
   "source": [
    "data,label = preprocess_audio(main_folder)\n",
    "print(\"Total audio files:\",len(data))\n",
    "print(\"Total labels:\",len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408, 408, 13, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=desired_shape),  # Specify the input shape (e.g., (num_frames, num_features, num_channels))\n",
    "    \n",
    "    # Convolutional layers\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten the output\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),  # Dropout layer to reduce overfitting\n",
    "    layers.Dense(num_classes, activation='softmax')  # Output layer with the number of classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 408, 13, 64)       640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 204, 6, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 204, 6, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 102, 3, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 102, 3, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 51, 1, 128)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6528)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               835712    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,058,437\n",
      "Trainable params: 1,058,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Use 'categorical_crossentropy' if one-hot encoding\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, label, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to avoid overfitting of model\n",
    "early_stop=EarlyStopping(monitor='val_accuracy',mode='max', verbose=1, patience=15, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "11/11 [==============================] - 4s 305ms/step - loss: 1.5676 - accuracy: 0.2725 - val_loss: 1.5559 - val_accuracy: 0.2195\n",
      "Epoch 2/30\n",
      "11/11 [==============================] - 3s 293ms/step - loss: 1.4891 - accuracy: 0.3215 - val_loss: 1.3990 - val_accuracy: 0.5366\n",
      "Epoch 3/30\n",
      "11/11 [==============================] - 3s 273ms/step - loss: 1.3457 - accuracy: 0.4687 - val_loss: 1.2392 - val_accuracy: 0.5122\n",
      "Epoch 4/30\n",
      "11/11 [==============================] - 3s 268ms/step - loss: 1.1479 - accuracy: 0.5913 - val_loss: 1.0032 - val_accuracy: 0.6098\n",
      "Epoch 5/30\n",
      "11/11 [==============================] - 3s 302ms/step - loss: 0.9284 - accuracy: 0.6730 - val_loss: 0.7844 - val_accuracy: 0.7561\n",
      "Epoch 6/30\n",
      "11/11 [==============================] - 3s 297ms/step - loss: 0.6444 - accuracy: 0.7793 - val_loss: 0.4373 - val_accuracy: 0.8537\n",
      "Epoch 7/30\n",
      "11/11 [==============================] - 3s 290ms/step - loss: 0.4745 - accuracy: 0.8392 - val_loss: 0.4075 - val_accuracy: 0.8049\n",
      "Epoch 8/30\n",
      "11/11 [==============================] - 3s 287ms/step - loss: 0.3523 - accuracy: 0.8774 - val_loss: 0.2861 - val_accuracy: 0.9024\n",
      "Epoch 9/30\n",
      "11/11 [==============================] - 3s 283ms/step - loss: 0.2079 - accuracy: 0.9346 - val_loss: 0.1731 - val_accuracy: 0.9268\n",
      "Epoch 10/30\n",
      "11/11 [==============================] - 3s 278ms/step - loss: 0.1364 - accuracy: 0.9537 - val_loss: 0.1206 - val_accuracy: 0.9512\n",
      "Epoch 11/30\n",
      "11/11 [==============================] - 3s 292ms/step - loss: 0.0890 - accuracy: 0.9700 - val_loss: 0.1116 - val_accuracy: 0.9756\n",
      "Epoch 12/30\n",
      "11/11 [==============================] - 3s 306ms/step - loss: 0.0677 - accuracy: 0.9837 - val_loss: 0.0510 - val_accuracy: 0.9756\n",
      "Epoch 13/30\n",
      "11/11 [==============================] - 3s 292ms/step - loss: 0.0458 - accuracy: 0.9946 - val_loss: 0.0648 - val_accuracy: 0.9512\n",
      "Epoch 14/30\n",
      "11/11 [==============================] - 3s 309ms/step - loss: 0.0245 - accuracy: 0.9946 - val_loss: 0.0746 - val_accuracy: 0.9756\n",
      "Epoch 15/30\n",
      "11/11 [==============================] - 3s 293ms/step - loss: 0.0174 - accuracy: 0.9973 - val_loss: 0.0434 - val_accuracy: 0.9512\n",
      "Epoch 16/30\n",
      "11/11 [==============================] - 3s 304ms/step - loss: 0.0150 - accuracy: 0.9946 - val_loss: 0.0748 - val_accuracy: 0.9756\n",
      "Epoch 17/30\n",
      "11/11 [==============================] - 3s 293ms/step - loss: 0.0136 - accuracy: 0.9973 - val_loss: 0.0282 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "11/11 [==============================] - 3s 274ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 0.9756\n",
      "Epoch 19/30\n",
      "11/11 [==============================] - 3s 276ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 0.9756\n",
      "Epoch 20/30\n",
      "11/11 [==============================] - 3s 279ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 0.9756\n",
      "Epoch 21/30\n",
      "11/11 [==============================] - 3s 283ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0669 - val_accuracy: 0.9756\n",
      "Epoch 22/30\n",
      "11/11 [==============================] - 3s 289ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0328 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "11/11 [==============================] - 4s 325ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 0.9756\n",
      "Epoch 24/30\n",
      "11/11 [==============================] - 3s 306ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0369 - val_accuracy: 0.9756\n",
      "Epoch 25/30\n",
      "11/11 [==============================] - 3s 316ms/step - loss: 0.0099 - accuracy: 0.9946 - val_loss: 0.0405 - val_accuracy: 0.9756\n",
      "Epoch 26/30\n",
      "11/11 [==============================] - 3s 288ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.0404 - val_accuracy: 0.9756\n",
      "Epoch 27/30\n",
      "11/11 [==============================] - 3s 307ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9756\n",
      "Epoch 28/30\n",
      "11/11 [==============================] - 3s 291ms/step - loss: 0.0148 - accuracy: 0.9973 - val_loss: 0.0623 - val_accuracy: 0.9756\n",
      "Epoch 29/30\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0639 - val_accuracy: 0.9756\n",
      "Epoch 30/30\n",
      "11/11 [==============================] - 3s 303ms/step - loss: 0.0091 - accuracy: 0.9973 - val_loss: 0.1365 - val_accuracy: 0.9756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241c266ea60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=36,callbacks=[early_stop], validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss & Accuracy of Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 72ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Training loss: 0.0193\n",
      "Training accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "print(f\"Training loss: {loss:.4f}\")\n",
    "print(f\"Training accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss & Accuracy of Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1365 - accuracy: 0.9756\n",
      "Testing loss: 0.1365\n",
      "Testing accuracy: 0.9756\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Testing loss: {loss:.4f}\")\n",
    "print(f\"Testing accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of testing audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_preprocess_audio(audio_file):\n",
    "        \n",
    "        Test_preprocess_data = []\n",
    "        original_audio, sr = sf.read(audio_file)\n",
    "        # Define the augmentation pipeline\n",
    "        augment1 = A.Compose([A.AddGaussianNoise(p=0.2)])\n",
    "        augment2 = A.Compose([A.TimeStretch(p=0.2)])\n",
    "        augment3 = A.Compose([A.PitchShift(p=0.2)])\n",
    "        augment4 = A.Compose([A.Shift(p=0.2)])\n",
    "        augment5 = A.Compose([A.TimeMask(p=0.2)])\n",
    "\n",
    "        # Apply augmentation to create augmented audio\n",
    "        augmented_audio1 = augment1(samples=original_audio, sample_rate=sr)\n",
    "        augmented_audio2 = augment2(samples=original_audio, sample_rate=sr)\n",
    "        augmented_audio3 = augment3(samples=original_audio, sample_rate=sr)\n",
    "        augmented_audio4 = augment4(samples=original_audio, sample_rate=sr)\n",
    "        augmented_audio5 = augment5(samples=original_audio, sample_rate=sr)\n",
    "        \n",
    "\n",
    "        # Perform feature extraction (e.g., MFCCs)\n",
    "        for audio in [original_audio, augmented_audio1, augmented_audio2, augmented_audio3, augmented_audio4, augmented_audio5]:\n",
    "                mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=num_mfcc_coefficients)\n",
    "\n",
    "                # Normalize the MFCCs (optional but recommended)\n",
    "                mfccs = (mfccs - np.mean(mfccs)) / np.std(mfccs)\n",
    "\n",
    "                # Reshape or pad the MFCCs to match the desired input shape\n",
    "                num_frames = mfccs.shape[1]\n",
    "                if num_frames < desired_shape[0]:\n",
    "                    mfccs = np.pad(mfccs, ((0, 0), (0, desired_shape[0] - num_frames)), mode='constant')\n",
    "                elif num_frames > desired_shape[0]:\n",
    "                    mfccs = mfccs[:, :desired_shape[0]]\n",
    "\n",
    "                # Append the preprocessed data and label\n",
    "                Test_preprocess_data.append(mfccs.T[:, :, np.newaxis])  # Transpose the data\n",
    "\n",
    "        return np.array(Test_preprocess_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the audio label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##['belly_pain', 'burping', 'discomfort', 'hungry', 'tired']\n",
    "def Predict_Label(audio_file):\n",
    "    processed_data = (Test_preprocess_audio(audio_file))\n",
    "    y_pred=model.predict(processed_data)\n",
    "    y_pred=np.argmax(y_pred,axis=1)\n",
    "    y_pred = int(np.median(y_pred))\n",
    "    print(y_pred)\n",
    "    if y_pred == 0:\n",
    "        print('belly_pain')\n",
    "    if y_pred == 1:\n",
    "        print('burping')\n",
    "    if y_pred == 2:\n",
    "        print('discomfort')\n",
    "    if y_pred == 3:\n",
    "        print('hungry')\n",
    "    if y_pred == 4:\n",
    "        print('tired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n",
      "1\n",
      "burping\n"
     ]
    }
   ],
   "source": [
    "Predict_Label(\"donateacry/burping/F24DE44B-762C-4149-AC92-96A5E57ED118-1430816949-1.0-m-04-bu.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "0\n",
      "belly_pain\n"
     ]
    }
   ],
   "source": [
    "Predict_Label(\"donateacry/belly_pain/549a46d8-9c84-430e-ade8-97eae2bef787-1430130772174-1.7-m-48-bp.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "3\n",
      "hungry\n"
     ]
    }
   ],
   "source": [
    "Predict_Label(\"donateacry/hungry/0f257dac-7d6f-4575-9192-e3b4dcd3d4ef-1430185441581-1.7-f-26-hu.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "2\n",
      "discomfort\n"
     ]
    }
   ],
   "source": [
    "Predict_Label(\"donateacry/discomfort/64acb345-a61e-4ef3-a5a6-cf83c04b83f1-1430058990597-1.7-m-72-dc.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "4\n",
      "tired\n"
     ]
    }
   ],
   "source": [
    "Predict_Label(\"donateacry/tired/7A22229D-06C2-4AAA-9674-DE5DF1906B3A-1436891957-1.1-m-72-ti.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Neonatal_cry_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"Neonatal_cry_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 87ms/step\n",
      "4\n",
      "tired\n"
     ]
    }
   ],
   "source": [
    "Predict_Label(\"donateacry/tired/7A22229D-06C2-4AAA-9674-DE5DF1906B3A-1436891957-1.1-m-72-ti.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
